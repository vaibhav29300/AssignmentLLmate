# -*- coding: utf-8 -*-
"""Test_Cases

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u2XOuZGazLH1Z8aMLqZWvwYDV8KVLQV2
"""

import unittest

class DataPipelineTest(unittest.TestCase):

    def test_data_ingestion(self):
        # Define a sample schema and data
        schema = StructType([
            StructField("order_id", StringType(), True),
            StructField("customer_id", StringType(), True),
            StructField("order_date", StringType(), True)
        ])

        # Simulate DataFrame for testing ingestion
        df = spark.createDataFrame([
            Row(order_id="O1001", customer_id="C123", order_date="2023-08-15"),
            Row(order_id="O1002", customer_id="C124", order_date="2023-08-16")
        ], schema)

        # Test the ingestion by checking row count and schema
        self.assertEqual(df.count(), 2)
        self.assertEqual(df.schema, schema)

    def test_data_transformation(self):
        # Create a sample DataFrame with duplicates
        df = spark.createDataFrame([
            Row(order_id="O1001", customer_id="C123"),
            Row(order_id="O1001", customer_id="C123")  # Duplicate row
        ])

        # Remove duplicates
        df_unique = df.dropDuplicates(["order_id"])

        # Test that duplicates are removed
        self.assertEqual(df_unique.count(), 1)

    def test_data_enrichment(self):
        # Sample customer DataFrame
        schema = StructType([
            StructField("customer_id", StringType(), True),
            StructField("first_name", StringType(), True),
            StructField("last_name", StringType(), True),
            StructField("address", StringType(), True)
        ])

        df = spark.createDataFrame([
            Row(customer_id="C123", first_name="Jane", last_name="Smith", address="Illinois")
        ], schema)

        # Enrichment: Add customer_name and region
        enriched_df = df.withColumn("customer_name", concat_ws(" ", col("first_name"), col("last_name"))) \
                        .withColumn("region", col("address"))

        # Test that columns were created and contain correct values
        self.assertEqual(enriched_df.filter(enriched_df.customer_name == "Jane Smith").count(), 1)
        self.assertEqual(enriched_df.filter(enriched_df.region == "Illinois").count(), 1)

    def test_data_loading(self):
        # Define a schema and data
        schema = StructType([
            StructField("order_id", StringType(), True),
            StructField("total_amount", FloatType(), True)
        ])

        df = spark.createDataFrame([
            Row(order_id="O1001", total_amount=250.75),
            Row(order_id="O1002", total_amount=150.50)
        ], schema)

        # Save to Parquet
        df.write.mode("overwrite").parquet("/content/test_orders_parquet")

        # Read back from Parquet and test row count
        df_loaded = spark.read.parquet("/content/test_orders_parquet")
        self.assertEqual(df_loaded.count(), 2)


# Run the tests
unittest.main(argv=[''], verbosity=2, exit=False)